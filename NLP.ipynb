{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDOt8JgNSFiTuWbeGGARFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhithanicky/Testproject/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc_GiHAq1GW2",
        "outputId": "a268930c-28c0-4c57-d436-b6c41aa97539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.46262479 0.\n",
            "  0.         0.         0.46262479 0.         0.46262479 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.37935895 0.         0.46262479]\n",
            " [0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.57735027 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.57735027 0.         0.         0.57735027\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.4472136  0.         0.         0.         0.4472136  0.\n",
            "  0.         0.         0.4472136  0.4472136  0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.24490668 0.         0.24490668\n",
            "  0.24490668 0.24490668 0.         0.24490668 0.         0.24490668\n",
            "  0.         0.24490668 0.         0.24490668 0.         0.\n",
            "  0.24490668 0.24490668 0.         0.         0.48981336 0.24490668\n",
            "  0.20082698 0.24490668 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "para = '''\"Beans. I was trying to explain to somebody as we were flying in, that’s corn.\"\n",
        "\"That’s beans. And they were very impressed at my agricultural knowledge.\"\n",
        "\"Please give it up for Amaury once again for that outstanding introduction.\"\n",
        "\"I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here.\"\"'''\n",
        "lem = WordNetLemmatizer()\n",
        "sent = nltk.sent_tokenize(para)\n",
        "corpus = []\n",
        "for i in range(len(sent)):\n",
        "    rev = re.sub('[^a-zA-Z]', ' ', sent[i])\n",
        "    rev = rev.lower()\n",
        "    rev = rev.split()\n",
        "    rev = [lem.lemmatize(word) for word in rev if word not in set(stopwords.words('english'))]\n",
        "    rev = ' '.join(rev)\n",
        "    corpus.append(rev)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "r=tf.fit_transform(corpus).toarray()\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text frequency inverse document frequency"
      ],
      "metadata": {
        "id": "JjcdqKzE1KSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BagOfWords"
      ],
      "metadata": {
        "id": "KqQy6G-Y2Tod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "para = '''\"Beans. I was trying to explain to somebody as we were flying in, that’s corn.\"\n",
        "\"That’s beans. And they were very impressed at my agricultural knowledge.\"\n",
        "\"Please give it up for Amaury once again for that outstanding introduction.\"\n",
        "\"I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here.\"\"'''\n",
        "lem = WordNetLemmatizer()\n",
        "sent = nltk.sent_tokenize(para)\n",
        "corpus = []\n",
        "for i in range(len(sent)):\n",
        "    rev = re.sub('[^a-zA-Z]', ' ', sent[i])\n",
        "    rev = rev.lower()\n",
        "    rev = rev.split()\n",
        "    rev = [lem.lemmatize(word) for word in rev if word not in set(stopwords.words('english'))]\n",
        "    rev = ' '.join(rev)\n",
        "    corpus.append(rev)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "tf=CountVectorizer()\n",
        "r=tf.fit_transform(corpus).toarray()\n",
        "print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omLfbd5n7_Ln",
        "outputId": "e7866fc7-f4b5-4008-823f-261bfade0f98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 2 1 1 1 0]]\n"
          ]
        }
      ]
    }
  ]
}